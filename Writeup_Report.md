# **Behavioral Cloning Project Report** 

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report

[//]: # (Image References)

[image1]: ./Images/Model_Arch.jpg "Model Architecture"
[image2]: ./Images/Strght_turn.jpg "Straight and trun "
[image3]: ./Images/Strght_turn_flipd.jpg "Straight and trun flipped"

## Rubric Points

### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* video.mp4 showing car driving around track1.
* writeup_report.md summarizing the results
* Track2 folder containing video (track2.mp4) driving around track2 and trained model (model.h5) on track2

#### 2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
```sh
python drive.py model.h5
```

#### 3. Submission code is usable and readable

The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

My model architecture `(lines #88-114 in model.py)` consists of three blocks of convolutional 
layers `(lines #94-107 in model.py)` with feature maps ranging from 10 to 150 as shown in the 
image below. Each block starts with a convolutional layer with a filter size 5x5 and stride 1 
followed by a MaxPooling layer with pool size of 2x2.The blocks end with a 1x1 convolutional 
layer to introduce nonlinearity (decision making) into the system and reduce feature maps. 

The model uses RELU as an activation function to introduce nonlinearity into the system. It 
finally joins with flattened layer which is fully connected to another layer with 128 neurons 
and finally ends in a prediction neuron. This neuron is used to predict steering angles. 

The training images used as input are cropped `(lines #)` and normalized `(lines #)` before passing 
into the model.

#### 2. Attempts to reduce overfitting in the model

Overfitting was not observed while training the model as validation loss was always 
found to be lower than training loss. However, dropout layers `(lines #)` were still used in the model 
architecture. The model was then validated on different datasets (provided by udacity as well as
new dataset generated locally). 

#### 3. Model parameter tuning

The hyperparameters were tuned according to the local machine as model was trained locally.
Initially a batch size of 128 was taken which was taking up too much memory. Batch size of 32
was found to be the best. The model used an adam optimizer and default learning rate of  
0.001 was found to be appropriate for training the model.

#### 4. Appropriate training data

Different training datasets were used. The best model performance was noticed on dataset
provided by Udacity. The training dataset was also complemented by flipping the images 
left to right (flipping about vertical line passing through center of camera view). 
 
More datasets were generated by keeping the vehicle in the middle of the road and also a 
reverse lap of the track was recorded for generalization.

### Model Architecture and Training Strategy

#### 1. Solution Design Approach

I started with a basic model which consisted of 1 convolutional layer followed by a fully 
connected layer containing 128 neurons. This was done to check how model processes images
provided for training. Initially, center images were used for training the model.

Once the model worked, images provided for training the model was split into training and 
validation sets to gauge model performance. As the validation loss and training loss was very 
high for this simple model, more convolutional layers were added to achieve sufficient model 
complexity (i.e. to reduce training and validation losses considerably).

The input images to the the model were cropped in order to remove irrelevant portion of the
image and hence reduce model training time. The images were also normalized before passing 
to the model in order to help optimizer converge to a minimum loss. The dropout layer was 
also added to the model in order to avoid model overfitting while training. 

The final model as shown below was driving well on straight roads but was facing problem on the 
turns. In order to resolve this problem, the model was supplied with left and right camera images
with a steering constant (steering angle) initially assigned to these images to keep the car of the curbs. 
This improved the model slightly. Hence, the steering constant was optimized and added/subtracted to 
corresponding steering values as required to keep the car off the curb for model during training. 

The model was performing well but was driving off the road towards left curb close to the bridge. 
The flipped images were then augmented to the dataset to reduce left turn bias and achieving a 
successful model which was able to drive autonomously around the track without going off.

#### 2. Final Model Architecture

The final model architecture (model.py lines 88-114) consisted of a 3 convolution blocks of convolutional
layers with the following layers and layer sizes as shown in the image below.

Here is a visualization of the architecture 

![alt text][image1]

#### 3. Creation of the Training Set & Training Process

I recorded a lap of the track to create a training dataset for my model. Then augmented the dataset with another
lap in same direction and one lap in opposite direction.

I also tried training my model on udacity provided dataset which i think had 3 straight and 2 reverse laps.

The image shown below is taken from the center camera while driving on straight road and turn. 

![alt text][image2]

The training data was further augmented by flipping the images and corresponding steering angles to increase size 
of training dataset and reduce bias towards left turn. Flipped images for driving on straight road and turn is 
shown below.

![alt text][image3]

The dataset was further augmented by using `ImageDataGenerator()` function from keras to augment original dataset.
Various functions like `rotation shift, rescaling,` etc. were utilised to augment the dataset on the fly using 
`.fit_generator()`. However, this was removed while training on GPU as it was running significantly slower and 
`.fit()` function was used to train the model. 

In order to counter above problem, python generator was used with `.fit_generator() `function . This generator was 
used to generate datasets in batches and hence not use too much memory for storing training datasets.  

Finally the dataset was shuffled and then 20% of the training dataset was utilised for validation. The validation 
datset was then used to gauge the performance of the model. The model achieved minimum training loss in 3 epochs 
and was able to drive around track one autonomously.

## Track Two

The model was trained on optional track2 using the dataset generated through udacity simulator. The model was 
trained using the same architecture as described above. Only change was that the model was trained for 5 epochs 
to reduce the training and validation losses. 

The final trained model was able to drive autonomously around track2 without getting of the road.

#### Shortcomings of the model:
Some of the shortcoming of the model observed were as follows:
* Although the trained model was able to drive around both the tracks but the steering was observed to be a lot
jittery especially on straight roads. 
* The car speed was fixed at 9 mph. It would be nice if we can supply throttle also as a parameter so that
model can gauge when to drive fast or slow it. 
* Currently two seperate models are trained to drive around both tracks individually. I am trying to train 
a single model which can drive around both the tracks.

